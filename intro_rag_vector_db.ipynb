{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae545bb0",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Vector Databases  \n",
    "### A Practical, End-to-End Walkthrough using ChromaDB\n",
    "\n",
    "Large Language Models (LLMs) are powerful reasoning engines, but they have a fundamental limitation:\n",
    "they **do not have access to private, proprietary, or up-to-date data**.\n",
    "\n",
    "By default, an LLM:\n",
    "- Relies only on its training data (with a fixed knowledge cutoff)\n",
    "- Cannot access internal documents or databases\n",
    "- May produce generic or outdated answers for factual questions\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** addresses this limitation by grounding LLM responses in external data at inference time.\n",
    "\n",
    "---\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG is an architecture that combines:\n",
    "1. **Information Retrieval** — finding relevant documents\n",
    "2. **Language Generation** — reasoning over retrieved content\n",
    "\n",
    "Instead of asking the model to “know everything”, we:\n",
    "- Retrieve the most relevant documents for a query\n",
    "- Inject those documents into the prompt\n",
    "- Force the model to answer *only using retrieved context*\n",
    "\n",
    "This makes answers:\n",
    "- More accurate\n",
    "- More explainable\n",
    "- Auditable and enterprise-ready\n",
    "\n",
    "---\n",
    "\n",
    "## Where is RAG Useful?\n",
    "\n",
    "RAG is particularly effective for:\n",
    "- Financial reports and earnings analysis\n",
    "- Internal knowledge bases\n",
    "- Policy and compliance documents\n",
    "- Research papers and technical documentation\n",
    "- Any domain where **correctness matters more than creativity**\n",
    "\n",
    "---\n",
    "\n",
    "## Why Vector Databases?\n",
    "\n",
    "To retrieve relevant documents efficiently, we need a way to compare **semantic similarity**, not just keywords.\n",
    "\n",
    "Vector databases (like **ChromaDB**) enable this by:\n",
    "- Storing documents as numerical embeddings\n",
    "- Performing fast similarity search in high-dimensional space\n",
    "- Scaling retrieval beyond simple keyword matching\n",
    "\n",
    "In this notebook, we use **ChromaDB** as a lightweight vector database to demonstrate this workflow end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f069e9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee9f6b",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Document Corpus\n",
    "\n",
    "We start by defining a small set of documents derived from JPMorgan Chase’s Q1 2023 earnings performance.\n",
    "\n",
    "Each document represents a **retrieval unit (chunk)** that the system can later search over.\n",
    "In real systems, these would typically be paragraphs or sections extracted from PDFs, filings, or internal reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd4130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"In Q1 2023, JPMorgan Chase reported net income of $12.6 billion, or $4.10 earnings per share, reflecting strong profitability.\",\n",
    "    \"Total net revenue for JPMorgan Chase in the first quarter of 2023 was $38.3 billion, up significantly year-over-year.\",\n",
    "    \"Net interest income was approximately $20.7 billion in Q1 2023, up roughly 49% driven by higher interest rates.\",\n",
    "    \"JPMorgan’s first-quarter 2023 return on equity was around 18%, and return on tangible common equity was about 23%.\",\n",
    "    \"Capital levels remained well above regulatory minimums in Q1 2023, with a common equity tier 1 (CET1) ratio near 13.8%.\",\n",
    "    \"Credit quality trends during the quarter remained stable with delinquency and charge-off rates largely unchanged.\",\n",
    "    \"Noninterest expenses for Q1 2023 included higher investments in technology and staffing to support long-term growth.\",\n",
    "    \"Segment performance in Q1 2023 showed resilience across consumer, commercial, and investment banking operations.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d37a8",
   "metadata": {},
   "source": [
    "## Step 3: Store Embeddings in a Vector Database (ChromaDB)\n",
    "\n",
    "Once embeddings are created, they are stored in a vector database.\n",
    "\n",
    "ChromaDB allows us to:\n",
    "- Bring our own embeddings\n",
    "- Persist vectors alongside raw text\n",
    "- Perform efficient similarity search at query time\n",
    "\n",
    "For simplicity, we use the Word2Vec embedding to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f831e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    epochs=200\n",
    ")\n",
    "\n",
    "def embed_text(text):\n",
    "    words = text.lower().split()\n",
    "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "doc_embeddings = np.vstack([embed_text(doc) for doc in documents])\n",
    "doc_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cada9357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.Client()\n",
    "\n",
    "existing_collections = [c.name for c in client.list_collections()]\n",
    "\n",
    "if \"rag_demo\" in existing_collections:\n",
    "    collection = client.get_collection(name=\"rag_demo\")\n",
    "else:\n",
    "    collection = client.create_collection(name=\"rag_demo\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    collection.add(\n",
    "        documents=[doc],\n",
    "        ids=[str(i)],\n",
    "        embeddings=[doc_embeddings[i]]\n",
    "    )\n",
    "\n",
    "collection.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e48c7",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Retrieve Relevant Documents for a User Query\n",
    "\n",
    "Given a user query:\n",
    "- The query is embedded using the same embedding model\n",
    "- The vector database returns the most similar documents\n",
    "- These documents form the factual context for generation\n",
    "\n",
    "At this point, we have **retrieval without generation**, which is already valuable on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2bd598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JPMorgan’s first-quarter 2023 return on equity was around 18%, and return on tangible common equity was about 23%.',\n",
       " 'In Q1 2023, JPMorgan Chase reported net income of $12.6 billion, or $4.10 earnings per share, reflecting strong profitability.',\n",
       " 'Capital levels remained well above regulatory minimums in Q1 2023, with a common equity tier 1 (CET1) ratio near 13.8%.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How strong is JPMorgan's capital position in Q1 2023?\"\n",
    "\n",
    "query_embedding = embed_text(query)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "retrieved_docs = results[\"documents\"][0]\n",
    "retrieved_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c4e89",
   "metadata": {},
   "source": [
    "## Step 5: LLM Generation Without RAG (Baseline)\n",
    "\n",
    "Before using retrieval, we first observe how the LLM responds **without any external context**.\n",
    "\n",
    "This serves as a baseline and highlights:\n",
    "- Generic language\n",
    "- Potentially outdated or vague answers\n",
    "- Lack of traceability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9b66d",
   "metadata": {},
   "source": [
    "#### OpenAI Client Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90f1ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7084791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of Q1 2023, JPMorgan Chase reported a strong capital position, with a Common Equity Tier 1 (CET1) capital ratio of 13.0%. This ratio reflects the bank's solid capital base relative to its risk-weighted assets, indicating a robust ability to absorb losses and support ongoing operations. Additionally, the bank maintained a well-capitalized status, which is crucial for regulatory compliance and financial stability. Overall, JPMorgan's capital position in Q1 2023 was considered strong, supporting its growth and resilience in the financial sector.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_no_rag = f\"\"\"\n",
    "Answer the following question:\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "response_no_rag = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_no_rag}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(response_no_rag.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577fa5fa",
   "metadata": {},
   "source": [
    "## Step 6: LLM Response With RAG\n",
    "\n",
    "Now we inject the retrieved documents into the prompt and explicitly instruct the model to answer **only using this context**.\n",
    "\n",
    "This is the core RAG step:\n",
    "- Retrieval + Generation\n",
    "- Same model, different behavior\n",
    "- Accuracy driven by documents, not model memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b09da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPMorgan's capital position in Q1 2023 is strong, with capital levels remaining well above regulatory minimums and a common equity tier 1 (CET1) ratio near 13.8%.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "prompt_rag = f\"\"\"\n",
    "Answer the question using ONLY the context below.\n",
    "If the answer is not present, say \"Not found in documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "response_rag = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_rag}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(response_rag.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab972ad8",
   "metadata": {},
   "source": [
    "## Conclusion: Why RAG Works\n",
    "\n",
    "This notebook demonstrates a key insight:\n",
    "\n",
    "> **The quality of factual answers depends more on retrieval than on model size or recency.**\n",
    "\n",
    "Without RAG:\n",
    "- The LLM relies on pretrained knowledge\n",
    "- Answers are generic, high-level, or outdated or sometime inaccurate (e.g. CET1 ratio in this example was incorrect for me without the RAG!)\n",
    "- There is no clear link between answer and source\n",
    "\n",
    "With RAG:\n",
    "- The LLM reasons over retrieved, domain-specific documents\n",
    "- Answers are precise, grounded, and explainable\n",
    "- The system becomes suitable for enterprise and analytical use cases\n",
    "\n",
    "Importantly, both responses use the **same language model**.\n",
    "The improvement comes entirely from **retrieval-augmented context**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "RAG transforms an LLM from a general conversational model into a **domain-aware reasoning system**.\n",
    "This makes it a foundational architecture for real-world applications where correctness matters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f1064",
   "metadata": {},
   "source": [
    "## Next Steps and Possible Improvements\n",
    "\n",
    "This notebook presents a minimal but complete RAG pipeline.\n",
    "In production systems, several enhancements are commonly applied:\n",
    "\n",
    "1. **Stronger Embedding Models**\n",
    "   - Sentence transformers or domain-specific embeddings\n",
    "   - Better semantic recall and robustness\n",
    "\n",
    "2. **Hybrid Retrieval**\n",
    "   - Combine vector similarity with keyword or BM25 search\n",
    "   - Improves precision for numbers, entities, and exact terms\n",
    "\n",
    "3. **Chunking and Overlap Strategies**\n",
    "   - Smarter document splitting improves retrieval quality\n",
    "   - Especially important for long reports and PDFs\n",
    "\n",
    "4. **Reranking Models**\n",
    "   - Use a secondary model to rerank retrieved documents\n",
    "   - Improves final context relevance\n",
    "\n",
    "5. **Graph-based RAG (GraphRAG)**\n",
    "   - Model relationships between entities and documents\n",
    "   - Enables multi-hop reasoning and structured retrieval\n",
    "\n",
    "6. **Evaluation and Monitoring**\n",
    "   - Measure retrieval quality (Recall@K)\n",
    "   - Track faithfulness and hallucination rates\n",
    "\n",
    "These techniques extend the same core idea demonstrated here:  \n",
    "**ground generation in reliable, retrievable data**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
